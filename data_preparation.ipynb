{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awards_players = pd.read_csv('data/awards_players.csv')\n",
    "df_coaches = pd.read_csv('data/coaches.csv')\n",
    "df_players_teams = pd.read_csv('data/players_teams.csv')\n",
    "df_players = pd.read_csv('data/players.csv')\n",
    "df_series_post = pd.read_csv('data/series_post.csv')\n",
    "df_teams = pd.read_csv('data/teams.csv')\n",
    "df_teams_post = pd.read_csv('data/teams_post.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop collumns that have null values or that have all the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def drop_columns_with_all_nan(df):\n",
    "    # Step 1: Check for columns with all NaN values\n",
    "    columns_to_drop = []\n",
    "    for col in df.columns:\n",
    "        unique_values = df[col].unique()\n",
    "        if len(unique_values) == 1 or all(pd.isna(x) for x in unique_values):\n",
    "            columns_to_drop.append(col)\n",
    "\n",
    "    # Step 2: Drop identified columns\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    # Step 3: Print the identified columns to be dropped\n",
    "    print(columns_to_drop)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lgID', 'divID', 'seeded', 'tmORB', 'tmDRB', 'tmTRB', 'opptmORB', 'opptmDRB', 'opptmTRB']\n",
      "0\n",
      "['lgID']\n",
      "0\n",
      "['lgIDWinner', 'lgIDLoser']\n",
      "0\n",
      "['firstseason', 'lastseason']\n",
      "0\n",
      "['lgID']\n",
      "0\n",
      "['lgID']\n",
      "0\n",
      "20\n",
      "['lgID']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "drop_columns_with_all_nan(df_teams)\n",
    "print(df_teams.duplicated().sum())\n",
    "df_teams.drop_duplicates(inplace=True)\n",
    "\n",
    "drop_columns_with_all_nan(df_teams_post)\n",
    "print(df_teams_post.duplicated().sum())\n",
    "df_teams_post.drop_duplicates(inplace=True)\n",
    "\n",
    "drop_columns_with_all_nan(df_series_post)\n",
    "print(df_series_post.duplicated().sum())\n",
    "df_series_post.drop_duplicates(inplace=True)\n",
    "\n",
    "drop_columns_with_all_nan(df_players)\n",
    "print(df_players.duplicated().sum())\n",
    "df_players.drop_duplicates(inplace=True)\n",
    "\n",
    "drop_columns_with_all_nan(df_players_teams)\n",
    "print(df_players_teams.duplicated().sum())\n",
    "df_players_teams.drop_duplicates(inplace=True)\n",
    "\n",
    "drop_columns_with_all_nan(df_coaches)\n",
    "print(df_coaches.duplicated().sum())\n",
    "print(df_coaches.duplicated(subset=['tmID', 'year']).sum())\n",
    "df_coaches.drop_duplicates(inplace=True)\n",
    "\n",
    "drop_columns_with_all_nan(df_awards_players)\n",
    "print(df_awards_players.duplicated().sum())\n",
    "df_awards_players.drop_duplicates(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregate data according with previous years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have the information about the year at the end of the playoff in each dataset, to avoid data leakage we will aggregate the data from the two previous years to the current year. And create new datasets by doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_back=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are new teams every year we decided to drop all the collumns as only the teams that played before had any relevant information. So as to not have any bias towards the teams that played before, all collumns were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jigs2\\Desktop\\AC_66\\data_preparation.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jigs2/Desktop/AC_66/data_preparation.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         df_teams_copy\u001b[39m.\u001b[39mloc[(df_teams_copy[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m year) \u001b[39m&\u001b[39m (df_teams_copy[\u001b[39m'\u001b[39m\u001b[39mtmID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m team), \u001b[39m'\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (df_teams[(df_teams[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (year \u001b[39m-\u001b[39m years_back)) \u001b[39m&\u001b[39m(df_teams[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m<\u001b[39m year) \u001b[39m&\u001b[39m (df_teams[\u001b[39m'\u001b[39m\u001b[39mtmID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m team)][\u001b[39m'\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jigs2/Desktop/AC_66/data_preparation.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m average_collumns:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jigs2/Desktop/AC_66/data_preparation.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m             df_teams_copy\u001b[39m.\u001b[39mloc[(df_teams_copy[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m year) \u001b[39m&\u001b[39m (df_teams_copy[\u001b[39m'\u001b[39m\u001b[39mtmID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m team), column] \u001b[39m=\u001b[39m df_teams[(df_teams[\u001b[39m'\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m (year \u001b[39m-\u001b[39;49m years_back)) \u001b[39m&\u001b[39;49m(df_teams[\u001b[39m'\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m<\u001b[39;49m year) \u001b[39m&\u001b[39;49m (df_teams[\u001b[39m'\u001b[39;49m\u001b[39mtmID\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m team)][column]\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jigs2/Desktop/AC_66/data_preparation.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m df_teams_copy\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mfirstRound\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msemis\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfinals\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mwon\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mlost\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfranchID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39marena\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mo_fgm\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_fga\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_ftm\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_fta\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_3pm\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_3pa\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_oreb\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_dreb\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_reb\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_asts\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_pf\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_stl\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_to\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_blk\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mo_pts\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_fgm\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_fga\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_ftm\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_fta\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_3pm\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_3pa\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_oreb\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_dreb\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_reb\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_asts\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_pf\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_stl\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_to\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_blk\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39md_pts\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mGP\u001b[39m\u001b[39m\"\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jigs2/Desktop/AC_66/data_preparation.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m df_teams_copy\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)   \n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:3751\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3749\u001b[0m \u001b[39m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   3750\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 3751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_bool_array(key)\n\u001b[0;32m   3753\u001b[0m \u001b[39m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   3754\u001b[0m \u001b[39m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   3755\u001b[0m is_single_key \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:3810\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   3809\u001b[0m indexer \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 3810\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_with_is_copy(indexer, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:3948\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3940\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_with_is_copy\u001b[39m(\u001b[39mself\u001b[39m: NDFrameT, indices, axis: Axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m   3941\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3942\u001b[0m \u001b[39m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   3943\u001b[0m \u001b[39m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3946\u001b[0m \u001b[39m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3947\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3948\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take(indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   3949\u001b[0m     \u001b[39m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   3950\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39m_get_axis(axis)\u001b[39m.\u001b[39mequals(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:3932\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[1;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[0;32m   3924\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   3925\u001b[0m         axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   3926\u001b[0m         \u001b[39mand\u001b[39;00m indices\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   3927\u001b[0m         \u001b[39mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   3928\u001b[0m         \u001b[39mand\u001b[39;00m is_range_indexer(indices, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m))\n\u001b[0;32m   3929\u001b[0m     ):\n\u001b[0;32m   3930\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 3932\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mtake(\n\u001b[0;32m   3933\u001b[0m     indices,\n\u001b[0;32m   3934\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_block_manager_axis(axis),\n\u001b[0;32m   3935\u001b[0m     verify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   3936\u001b[0m     convert_indices\u001b[39m=\u001b[39;49mconvert_indices,\n\u001b[0;32m   3937\u001b[0m )\n\u001b[0;32m   3938\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\managers.py:963\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[0;32m    960\u001b[0m     indexer \u001b[39m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[39m=\u001b[39mverify)\n\u001b[0;32m    962\u001b[0m new_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 963\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreindex_indexer(\n\u001b[0;32m    964\u001b[0m     new_axis\u001b[39m=\u001b[39;49mnew_labels,\n\u001b[0;32m    965\u001b[0m     indexer\u001b[39m=\u001b[39;49mindexer,\n\u001b[0;32m    966\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    967\u001b[0m     allow_dups\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    968\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    969\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\managers.py:764\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    761\u001b[0m new_mgr \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfrom_blocks(new_blocks, new_axes)\n\u001b[0;32m    762\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    763\u001b[0m     \u001b[39m# We can avoid the need to rebuild these\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     new_mgr\u001b[39m.\u001b[39m_blknos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblknos\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m    765\u001b[0m     new_mgr\u001b[39m.\u001b[39m_blklocs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblklocs\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m new_mgr\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Replace 'N' and 'Y' in the 'playoff' column with 0 and 1\n",
    "\n",
    "df_teams['playoff'] = df_teams['playoff'].map({'N': 0, 'Y': 1})\n",
    "df_teams.sort_values(by=['year', 'tmID'], ascending=[True, True], inplace=True)\n",
    "\n",
    "years = df_teams['year'].unique()\n",
    "teams = df_teams['tmID'].unique()\n",
    "\n",
    "df_teams_copy = df_teams.copy()\n",
    "\n",
    "average_collumns = [\"homeW\",\"homeL\",\"awayW\",\"awayL\",\"confW\",\"confL\",\"min\",\"attend\"]\n",
    "\n",
    "for year in years:\n",
    "    for team in teams:\n",
    "        df_teams_copy.loc[(df_teams_copy['year'] == year) & (df_teams_copy['tmID'] == team), 'num_playoff_appearances'] = df_teams[(df_teams['year'] >= (year - years_back)) & (df_teams['year'] < year) & (df_teams['tmID'] == team)]['playoff'].mean()\n",
    "        df_teams_copy.loc[(df_teams_copy['year'] == year) & (df_teams_copy['tmID'] == team), 'mean_won'] = (df_teams[(df_teams['year'] >= (year - years_back)) &(df_teams['year'] < year) & (df_teams['tmID'] == team)]['won']).mean()\n",
    "        df_teams_copy.loc[(df_teams_copy['year'] == year) & (df_teams_copy['tmID'] == team), 'mean_lost'] = (df_teams[(df_teams['year'] >= (year - years_back)) &(df_teams['year'] < year) & (df_teams['tmID'] == team)]['lost']).mean()\n",
    "        df_teams_copy.loc[(df_teams_copy['year'] == year) & (df_teams_copy['tmID'] == team), 'rank'] = (df_teams[(df_teams['year'] >= (year - years_back)) &(df_teams['year'] < year) & (df_teams['tmID'] == team)]['rank']).mean()\n",
    "        for column in average_collumns:\n",
    "            df_teams_copy.loc[(df_teams_copy['year'] == year) & (df_teams_copy['tmID'] == team), column] = df_teams[(df_teams['year'] >= (year - years_back)) &(df_teams['year'] < year) & (df_teams['tmID'] == team)][column].mean()\n",
    " \n",
    "         \n",
    "df_teams_copy.drop(columns=['firstRound', 'semis', 'finals',\"won\",\"lost\", 'franchID', 'name', 'arena', \"o_fgm\",\"o_fga\",\"o_ftm\",\"o_fta\",\"o_3pm\",\"o_3pa\",\"o_oreb\",\"o_dreb\",\"o_reb\",\"o_asts\",\"o_pf\",\"o_stl\",\"o_to\",\"o_blk\",\"o_pts\",\"d_fgm\",\"d_fga\",\"d_ftm\",\"d_fta\",\"d_3pm\",\"d_3pa\",\"d_oreb\",\"d_dreb\",\"d_reb\",\"d_asts\",\"d_pf\",\"d_stl\",\"d_to\",\"d_blk\",\"d_pts\",\"GP\"], inplace=True)\n",
    "df_teams_copy.fillna(0, inplace=True)   \n",
    "df_teams_copy.to_csv('data/teams_processed.csv', index=False)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "categorical_features = ['confID']\n",
    "for feature in categorical_features:\n",
    "    onehotarray = encoder.fit_transform(df_teams_copy[[feature]]).toarray()\n",
    "    items = [f'{feature}_{item}' for item in encoder.categories_[0]]\n",
    "    df_teams_copy[items] = onehotarray\n",
    "df_teams_copy=df_teams_copy.drop(categorical_features, axis=1)\n",
    "\n",
    "df_teams_copy.to_csv('data/teams_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams_post_copy = pd.DataFrame(columns=[\"year\", \"tmID\", \"W\", \"L\"])\n",
    "sum_collumns = [\"W\", \"L\"]\n",
    "for year in years:\n",
    "    for team in teams:\n",
    "        for column in sum_collumns:\n",
    "            w=df_teams_post[(df_teams_post['year'] >= (year - years_back)) &(df_teams_post['year'] < year) & (df_teams_post['tmID'] == team)][column].mean()\n",
    "            l=df_teams_post[(df_teams_post['year'] >= (year - years_back)) &(df_teams_post['year'] < year) & (df_teams_post['tmID'] == team)][column].mean()\n",
    "            new_row = {'year':year, 'tmID':team, column:w}\n",
    "            df_teams_post_copy = pd.concat([df_teams_post_copy, pd.DataFrame([new_row])], ignore_index=True)\n",
    "df_teams_post_copy.fillna(0, inplace=True)\n",
    "df_teams_post_copy.to_csv('data/teams_post_processed.csv', index=False)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### series post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each team some the number of win and losses they had in the year. For each year we then agregate the information from the previous two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_team_stats = df_series_post.groupby([\"year\", \"tmIDWinner\"])[[\"W\"]].sum().reset_index()\n",
    "winning_team_stats.columns = [\"year\", \"tmID\", \"total_wins\"]\n",
    "\n",
    "losing_team_stats = df_series_post.groupby([\"year\", \"tmIDLoser\"])[[\"L\"]].sum().reset_index()\n",
    "losing_team_stats.columns = [\"year\", \"tmID\", \"total_losses\"]\n",
    "\n",
    "# Merge the winning and losing team statistics\n",
    "team_stats = winning_team_stats.merge(losing_team_stats, on=[\"year\", \"tmID\"], how=\"outer\").fillna(0)\n",
    "team_stats_copy = pd.DataFrame(columns=[\"year\", \"tmID\", \"total_wins\", \"total_losses\"])\n",
    "for year in years:\n",
    "    for team in teams:\n",
    "        total_wins = team_stats[(team_stats['year'] >= (year - years_back)) &(team_stats['year'] < year) & (team_stats['tmID'] == team)]['total_wins'].mean()\n",
    "        total_losses = team_stats[(team_stats['year'] >= (year - years_back)) &(team_stats['year'] < year) & (team_stats['tmID'] == team)]['total_losses'].mean()\n",
    "        new_row = {'year': year, 'tmID': team, 'total_wins': total_wins, 'total_losses': total_losses}\n",
    "        team_stats_copy = pd.concat([team_stats_copy, pd.DataFrame([new_row])], ignore_index=True)\n",
    "team_stats_copy.fillna(0, inplace=True)\n",
    "team_stats_copy.to_csv('data/series_post_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each player calculate the statistics of the previous two years, independently of the team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1876 entries, 0 to 1875\n",
      "Data columns (total 42 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   playerID            1876 non-null   object\n",
      " 1   year                1876 non-null   int64 \n",
      " 2   stint               1876 non-null   int64 \n",
      " 3   tmID                1876 non-null   object\n",
      " 4   GP                  1876 non-null   int64 \n",
      " 5   GS                  1876 non-null   int64 \n",
      " 6   minutes             1876 non-null   int64 \n",
      " 7   points              1876 non-null   int64 \n",
      " 8   oRebounds           1876 non-null   int64 \n",
      " 9   dRebounds           1876 non-null   int64 \n",
      " 10  rebounds            1876 non-null   int64 \n",
      " 11  assists             1876 non-null   int64 \n",
      " 12  steals              1876 non-null   int64 \n",
      " 13  blocks              1876 non-null   int64 \n",
      " 14  turnovers           1876 non-null   int64 \n",
      " 15  PF                  1876 non-null   int64 \n",
      " 16  fgAttempted         1876 non-null   int64 \n",
      " 17  fgMade              1876 non-null   int64 \n",
      " 18  ftAttempted         1876 non-null   int64 \n",
      " 19  ftMade              1876 non-null   int64 \n",
      " 20  threeAttempted      1876 non-null   int64 \n",
      " 21  threeMade           1876 non-null   int64 \n",
      " 22  dq                  1876 non-null   int64 \n",
      " 23  PostGP              1876 non-null   int64 \n",
      " 24  PostGS              1876 non-null   int64 \n",
      " 25  PostMinutes         1876 non-null   int64 \n",
      " 26  PostPoints          1876 non-null   int64 \n",
      " 27  PostoRebounds       1876 non-null   int64 \n",
      " 28  PostdRebounds       1876 non-null   int64 \n",
      " 29  PostRebounds        1876 non-null   int64 \n",
      " 30  PostAssists         1876 non-null   int64 \n",
      " 31  PostSteals          1876 non-null   int64 \n",
      " 32  PostBlocks          1876 non-null   int64 \n",
      " 33  PostTurnovers       1876 non-null   int64 \n",
      " 34  PostPF              1876 non-null   int64 \n",
      " 35  PostfgAttempted     1876 non-null   int64 \n",
      " 36  PostfgMade          1876 non-null   int64 \n",
      " 37  PostftAttempted     1876 non-null   int64 \n",
      " 38  PostftMade          1876 non-null   int64 \n",
      " 39  PostthreeAttempted  1876 non-null   int64 \n",
      " 40  PostthreeMade       1876 non-null   int64 \n",
      " 41  PostDQ              1876 non-null   int64 \n",
      "dtypes: int64(40), object(2)\n",
      "memory usage: 615.7+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1303 entries, 1 to 1871\n",
      "Data columns (total 41 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   playerID            1303 non-null   object \n",
      " 1   year                1303 non-null   int64  \n",
      " 2   tmID                1303 non-null   object \n",
      " 3   GP                  1303 non-null   float64\n",
      " 4   GS                  1303 non-null   float64\n",
      " 5   minutes             1303 non-null   float64\n",
      " 6   points              1303 non-null   float64\n",
      " 7   oRebounds           1303 non-null   float64\n",
      " 8   dRebounds           1303 non-null   float64\n",
      " 9   rebounds            1303 non-null   float64\n",
      " 10  assists             1303 non-null   float64\n",
      " 11  steals              1303 non-null   float64\n",
      " 12  blocks              1303 non-null   float64\n",
      " 13  turnovers           1303 non-null   float64\n",
      " 14  PF                  1303 non-null   float64\n",
      " 15  fgAttempted         1303 non-null   float64\n",
      " 16  fgMade              1303 non-null   float64\n",
      " 17  ftAttempted         1303 non-null   float64\n",
      " 18  ftMade              1303 non-null   float64\n",
      " 19  threeAttempted      1303 non-null   float64\n",
      " 20  threeMade           1303 non-null   float64\n",
      " 21  dq                  1303 non-null   float64\n",
      " 22  PostGP              1303 non-null   float64\n",
      " 23  PostGS              1303 non-null   float64\n",
      " 24  PostMinutes         1303 non-null   float64\n",
      " 25  PostPoints          1303 non-null   float64\n",
      " 26  PostoRebounds       1303 non-null   float64\n",
      " 27  PostdRebounds       1303 non-null   float64\n",
      " 28  PostRebounds        1303 non-null   float64\n",
      " 29  PostAssists         1303 non-null   float64\n",
      " 30  PostSteals          1303 non-null   float64\n",
      " 31  PostBlocks          1303 non-null   float64\n",
      " 32  PostTurnovers       1303 non-null   float64\n",
      " 33  PostPF              1303 non-null   float64\n",
      " 34  PostfgAttempted     1303 non-null   float64\n",
      " 35  PostfgMade          1303 non-null   float64\n",
      " 36  PostftAttempted     1303 non-null   float64\n",
      " 37  PostftMade          1303 non-null   float64\n",
      " 38  PostthreeAttempted  1303 non-null   float64\n",
      " 39  PostthreeMade       1303 non-null   float64\n",
      " 40  PostDQ              1303 non-null   float64\n",
      "dtypes: float64(38), int64(1), object(2)\n",
      "memory usage: 427.5+ KB\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "df_players_teams_copy = df_players_teams.copy()\n",
    "\n",
    "average_collumns = [\"GP\",\"GS\",\"minutes\",\"points\",\"oRebounds\",\"dRebounds\",\"rebounds\",\"assists\",\"steals\",\"blocks\",\"turnovers\",\"PF\",\"fgAttempted\",\"fgMade\",\"ftAttempted\",\"ftMade\",\"threeAttempted\",\"threeMade\",\"dq\",\"PostGP\",\"PostGS\",\"PostMinutes\",\"PostPoints\",\"PostoRebounds\",\"PostdRebounds\",\"PostRebounds\",\"PostAssists\",\"PostSteals\",\"PostBlocks\",\"PostTurnovers\",\"PostPF\",\"PostfgAttempted\",\"PostfgMade\",\"PostftAttempted\",\"PostftMade\",\"PostthreeAttempted\",\"PostthreeMade\",\"PostDQ\"]\n",
    "for year in years:\n",
    "    for team in teams:\n",
    "        players = df_players_teams[(df_players_teams['year'] == year) & (df_players_teams['tmID'] == team)]['playerID'].unique()\n",
    "        for player in players:\n",
    "            for column in average_collumns:\n",
    "                df_players_teams_copy.loc[(df_players_teams_copy['year'] == year) & (df_players_teams_copy['tmID'] == team) & (df_players_teams_copy['playerID'] == player), column] = df_players_teams[(df_players_teams['year'] >= (year - years_back)) &(df_players_teams['year'] < year)  & (df_players_teams['playerID'] == player)][column].mean()\n",
    "\n",
    "df_players_teams_copy.dropna(inplace=True)\n",
    "df_players_teams_copy.drop(columns=['stint'], inplace=True)\n",
    "df_players_teams_copy.to_csv('data/players_teams_processed.csv', index=False)\n",
    "\n",
    "print(df_players_teams.info(), df_players_teams_copy.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coaches_copy = df_coaches.copy()\n",
    "\n",
    "average_collumns = [\"won\",\"lost\",\"post_wins\",\"post_losses\"]\n",
    "for year in years:\n",
    "    for team in teams:\n",
    "        coaches= df_coaches[(df_coaches['year'] == year) & (df_coaches['tmID'] == team)]['coachID'].unique()\n",
    "        for coach in coaches:\n",
    "            for column in average_collumns:\n",
    "                df_coaches_copy.loc[(df_coaches_copy['year'] == year) & (df_coaches_copy['tmID'] == team) & (df_coaches_copy['coachID'] == coach), column] = df_coaches[(df_coaches['year'] >= (year - years_back)) &(df_coaches['year'] < year) & (df_coaches['coachID'] == coach)][column].mean()         \n",
    "\n",
    "df_coaches_copy.fillna(0, inplace=True)\n",
    "df_coaches_copy.to_csv('data/coaches_processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeOutliersMean(data, column):\n",
    "    #change the outliers with the mean\n",
    "    #SFinding the IQR\n",
    "    percentile25 = data[column].quantile(0.25)\n",
    "    percentile75 = data[column].quantile(0.75)\n",
    "    #Finding the upper and lower limits\n",
    "    iqr = percentile75 - percentile25\n",
    "    upper_limit = percentile75 + 1.5 * iqr\n",
    "    lower_limit = percentile25 - 1.5 * iqr\n",
    "\n",
    "    # Find outliers\n",
    "    outliers = data[(data[column] > upper_limit) | (data[column] < lower_limit)]\n",
    "\n",
    "    # Replace outliers with the mean of the 'height' column\n",
    "    mean_height = data[column].mean()\n",
    "    data.loc[outliers.index, column] = mean_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop players from whom we have no information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players = df_players[~((df_players['pos'].isna()) &\n",
    "                            (df_players['height'] == 0.0) &\n",
    "                            (df_players['weight'] == 0) &\n",
    "                            (df_players['college'].isna()) &\n",
    "                            (df_players['collegeOther'].isna()) &\n",
    "                            (df_players['birthDate'] == \"0000-00-00\") &\n",
    "                            (df_players['deathDate'] == \"0000-00-00\"))]\n",
    "changeOutliersMean(df_players, 'height')\n",
    "changeOutliersMean(df_players, 'weight')\n",
    "df_players.to_csv('data/players_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_counts = df_awards_players.groupby([\"year\", \"playerID\"]).size().reset_index(name=\"award_count\")\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "award_counts_copy = pd.DataFrame(columns=[\"year\", \"playerID\", \"award_count\"])\n",
    "players = df_awards_players['playerID'].unique()\n",
    "# Iterate over all possible combinations of \"year\" and \"player\" and add them to the DataFrame\n",
    "for year in years:\n",
    "    for player in players:\n",
    "        award_count = award_counts[(award_counts['year'] >= (year - years_back)) & (award_counts['year'] < year) & (award_counts['playerID'] == player)]['award_count'].sum()\n",
    "        new_row = {\"year\": year, \"playerID\": player, \"award_count\": award_count}\n",
    "        award_counts_copy = pd.concat([award_counts_copy, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "award_counts_copy.dropna(inplace=True)\n",
    "award_counts_copy.to_csv('data/awards_players_processed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
